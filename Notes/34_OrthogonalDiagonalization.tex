\documentclass[xcolor=dvipsnames,aspectratio=169,t]{beamer}
  % t means frames are vertically centered to the top
\usepackage{slides-header}
\title{Orthogonal Diagonalization and Symmetric Matrices}

\begin{document}
\maketitle

\begin{frame}{Diagonalizing with an Orthonormal Basis}
  \bigskip
  
  \textbf{\colorb{Thm.}}
  An $n\times n$ matrix $A$ is \alert{diagonalizable} if and only if
  \smallskip
  
  \qquad \quad there exist $n$ linearly independent eigenvectors.
  \smallskip
  
  \pause
  \qquad \quad $\Leftrightarrow$ there is a \colorb{basis} for $\mathbb{R}^n$ consisting of eigenvectors.
  
  \vspace*{1.5em}
  
  \textbf{\colorb{Ques.}}
  When does there exist an \alert{orthonormal basis} for $\mathbb{R}^n$ consisting of eigenvectors?
  
\end{frame}


\begin{frame}{Symmetric Matrix}
  \begin{definition}
    A \alert{symmetric matrix} is a square matrix $A$ such that $A^T=A$.
    \bi
    \ii Entries on the main diagonal can be anything.
    \ii Entries above and below the main diagonal come in mirrored pairs, $a_{ij} = a_{ji}$.
    \ei
  \end{definition}


Here are some examples of symmetric matrices:
\[ \begin{bmatrix} 1 & 2 \\ 2 & - 4 \end{bmatrix} \quad \mbox{and} \quad \begin{bmatrix} 1 & -2 & 7 \\  -2 & 12 & 5 \\ 7 & 5 & -17 \end{bmatrix}. \]

Here are some examples of matrices that are not symmetric:
\[ \begin{bmatrix} 1 & 2 \\ -2 & - 4 \end{bmatrix} \quad \mbox{and} \quad \begin{bmatrix} 1 & 0 & -1 \\  0 & 1 & 0 \\ 0 & -1 & 1 \end{bmatrix}. \]

\end{frame}

\begin{frame}{Review: Diagonalizing a Matrix}
\smallskip

Diagonalize the matrix $A = \begin{bmatrix} 6 & -2 & -1 \\ -2 & 6 & -1 \\ -1 & -1 & 5 \end{bmatrix}$.
\medskip

\bb
\ii Find the eigenvalues of $A$ by solving $\det(A - \lambda I) = 0$. \alert{We have $\lambda = 8, 6, 3$.}
\medskip

\pause
\ii If possible, find $n$ linearly independent eigenvectors. 
\medskip

\alert{We have $\v_1 = \begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix}$, $\v_2 = \begin{bmatrix} -1 \\ -1 \\ 2 \end{bmatrix}$, and $\v_3 = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}$ corresponding to $\lambda_1 = 8$, $\lambda_2 = 6$ and $\lambda_3 = 3$, respectively.}

\pause
\ii Then $A=PDP^{-1}$. \alert{We have  $P = \begin{bmatrix} -1 & -1 & 1 \\ 1 & -1 & 1 \\ 0 & 2 & 1 \end{bmatrix}$ and $D = \begin{bmatrix} 8 & 0 & 0 \\ 0 & 6 & 0 \\ 0 & 0 & 3 \end{bmatrix}$.}
\ee

\end{frame}

\begin{frame}{Choosing An Orthonormal Basis}
\smallskip

We have $\v_1 = \begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix}$, $\v_2 = \begin{bmatrix} -1 \\ -1 \\ 2 \end{bmatrix}$, and $\v_3 = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}$ corresponding to $\lambda_1 = 8$, $\lambda_2 = 6$ and $\lambda_3 = 3$, respectively.
\bigskip

\pause
We can instead construct column vectors for $P$ that are \alert{orthonormal}:
\medskip

We have \alert{$\u_1 = \begin{bmatrix} -1/\sqrt{2} \\ 1/\sqrt{2} \\ 0 \end{bmatrix}$}, \alert{$\u_2 = \begin{bmatrix} -1/\sqrt{6} \\ -1/\sqrt{6} \\ 2/\sqrt{6} \end{bmatrix}$}, and \alert{$\u_3 = \begin{bmatrix} 1/\sqrt{3} \\ 1/\sqrt{3} \\ 1/\sqrt{3} \end{bmatrix}$} corresponding to $\lambda_1 = 8$, $\lambda_2 = 6$ and $\lambda_3 = 3$, respectively.
\bigskip

\pause
Now we have $P = \begin{bmatrix} -1/\sqrt{2} & -1/\sqrt{6} & 1/\sqrt{3} \\ 1/\sqrt{2} & -1/\sqrt{6} & 1/\sqrt{3} \\ 0 & 2/\sqrt{6} & 1/\sqrt{3} \end{bmatrix}$ and $D = \begin{bmatrix} 8 & 0 & 0 \\ 0 & 6 & 0 \\ 0 & 0 & 3 \end{bmatrix}$.

\end{frame}

\begin{frame}{Orthogonal Matrices}
  \begin{definition}
    A matrix $P$ is called \alert{orthogonal} if it is a square matrix with \alert{orthonormal columns}.
    If $P$ is an orthogonal matrix, we have shown that $P^TP= I$ and $PP^T=I$, therefore $P^{-1}=P^T$.
  \end{definition}


\[ A = PDP^{-1} = PDP^T \]

\[ \begin{bmatrix} 6 & -2 & -1 \\ -2 & 6 & -1 \\ -1 & -1 & 5 \end{bmatrix}  = \begin{bmatrix} -1/\sqrt{2} & -1/\sqrt{6} & 1/\sqrt{3} \\ 1/\sqrt{2} & -1/\sqrt{6} & 1/\sqrt{3} \\ 0 & 2/\sqrt{6} & 1/\sqrt{3} \end{bmatrix} \begin{bmatrix} 8 & 0 & 0 \\ 0 & 6 & 0 \\ 0 & 0 & 3 \end{bmatrix}
\begin{bmatrix} -1/\sqrt{2} & 1/\sqrt{2} & 0 \\ -1/\sqrt{6} & -1/\sqrt{6} & 2/\sqrt{6} \\  1/\sqrt{3} & 1/\sqrt{3} & 1/\sqrt{3} \end{bmatrix} \]
 
\end{frame}

\begin{frame}{Orthogonality of Eigenvectors}
  \begin{theorem}
  If $A$ is \colorb{symmetric}, then any two eigenvectors from different eigenspaces are \alert{orthogonal}.
  \end{theorem}
  \bigskip

  \pause
  \blue{Proof.}\smallskip

  Let $\v_1$ and $\v_2$ be eigenvectors that correspond to different eigenvalues $\lambda_1$ and $\lambda_2$, respectively. We need to show that $\v_1 \cdot \v_2 = 0$. 
  \pause
  \begin{align*}
  \lambda_1 \v_1 \cdot \v_2 &= (A \v_1) \cdot \v_2 = (A \v_1)^T  \v_2  \ \ \ \ \mbox{by def of eigenvector} \\
  \onslide<4->{
  &=  \v_1^TA^T \v_2 =  \v_1^T A \v_2 \ \ \ \ \ \ \mbox{since $A$ is \colorb{symmetric}} \\
  }
  \onslide<5->{
  &= \v_1^T \lambda_2 \v_2 =\lambda_2 \v_1 \cdot \v_2. \ \ \ \ \mbox{by def of eigenvector}
  }
  \end{align*}

  \onslide<6->{
  Therefore, $(\lambda_1 - \lambda_2)  \v_1 \cdot \v_2 = 0$, which implies  $\v_1 \cdot \v_2 = 0$ since we assumed $\lambda_1$ and $\lambda_2$ are not equal.
  \hfill\blue{\qed}
  }
\end{frame}


\begin{frame}{Orthogonally Diagonalizable}
  \begin{definition}
    An $n \times n$ matrix $A$ is \alert{orthogonally diagonalizable} if there are an \blue{orthogonal} matrix $P$ and a \blue{diagonal} matrix $D$ such that $A = PDP^T$.
  \end{definition}

  \pause
  \begin{theorem}[The Spectral Theorem]
  An $n \times n$ matrix $A$ is \alert{orthogonally diagonalizable} if and only if $A$ is a \alert{symmetric} matrix.
  \end{theorem}

  \pause
  \blue{Proof.}\smallskip
  
  ($\Rightarrow$) Assume that $A$ is orthogonally diagonalizable. Then we know $A=PDP^T$.
  \pause
  Taking the transpose of both sides we have
  \vspace*{-.5em}
  \[ A^T = (PDP^T)^T = (P^T)^TD^TP^T = PDP^T = A.\]
  \vspace*{-1.75em}
  
  Since $A^T=A$, we see that $A$ is a symmetric matrix.
  \medskip

  \pause
  ($\Leftarrow$) Converse is \alert{difficult}!  Hard part: dim of each eigenspace equals the algebraic multiplicity.
  \hfill\blue{\qed}
\end{frame}


\begin{frame}{Example}
  \smallskip

  \alert{Orthogonally diagonalize} the matrix $A = \begin{bmatrix} 5 & 8 & -4 \\ 8 & 5 & -4 \\ -4 & -4 & -1 \end{bmatrix}$.
  \bigskip

  \pause
  \bb
  \ii First we find the eigenvalues: $\lambda_1 = -3$ (with multiplicity 2) and $\lambda_2 = 15$.
  \medskip

  \pause
  \ii Find a basis for the eigenspace of each eigenvector.

  \[ \lambda_1 = -3 \mbox{\ \ has \ \ } \left\{  \begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix} , \begin{bmatrix} 1 \\ 0 \\ 2 \end{bmatrix} \right\} \quad \mbox{and} \quad \lambda_2 = 15 \mbox{\ \ has \ \ }  \left\{\begin{bmatrix} -2 \\ -2 \\ 1 \end{bmatrix}\right\} \]

  \pause
  \ii Using the Gram--Schmidt process, find an orthogonal basis for the eigenspaces.
  \ee

\end{frame}


\begin{frame}{Making Things Orthogonal}
  \smallskip
  
  We have $\w_1 = \begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix}$, $\w_2 = \begin{bmatrix} 1 \\ 0 \\ 2 \end{bmatrix}$, and $\w_3 =  \begin{bmatrix} -2 \\ -2 \\ 1 \end{bmatrix}$.
  \bigskip

  \bi
  \ii $\w_1$ and $\w_2$ are NOT orthogonal. We set $\v_1 = \w_1$.
  \medskip

  \pause
  \ii We find the component of $\w_2$ orthogonal to $\v_1$
  \[ \proj_{\v_1} \w_2 = \frac{\w_2 \cdot \v_1}{\v_1 \cdot \v_1} \v_1 = \frac{-1}{2}  \begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix} = \begin{bmatrix} 1/2 \\ -1/2 \\ 0 \end{bmatrix} \]
  Our next vector is $\v_2 = \w_2 - \proj_{\v_1} \w_2 = \begin{bmatrix} 1/2 \\ 1/2 \\ 2 \end{bmatrix}$.

  \pause
  \ii $\w_3$ is orthogonal to $\v_1$ and $\v_2$, so we set $\v_3 = \w_3$.
  \ei
\end{frame}


\begin{frame}{Normalizing the Vectors}
  \smallskip

  We now have orthogonal eigenvectors 
  $\v_1 = \begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix}$,
  $\v_2 = \begin{bmatrix} 1/2 \\ 1/2 \\ 2 \end{bmatrix}$, and 
  $\v_3 =  \begin{bmatrix} -2 \\ -2 \\ 1 \end{bmatrix}$.
  \bigskip

  \pause
  \bb
  \addtocounter{enumi}{3}
  \ii Normalize each column vector to find possible columns of $P$.

  \[ P = \begin{bmatrix} -\frac{1}{\sqrt{2}} & \sqrt{2}/6 & -2/3 \\
  \frac{1}{\sqrt{2}} & \sqrt{2}/6 & -2/3\\
  0 & 2\sqrt{2}/3 & 1/3 \end{bmatrix} \]
  \ee


  We can check that $A=PDP^T$:

  \[ A = \begin{bmatrix} 5 & 8 & -4 \\ 8 & 5 & -4 \\ -4 & -4 & -1 \end{bmatrix} =  
  \begin{bmatrix} -\frac{1}{\sqrt{2}} & \sqrt{2}/6 & -2/3 \\ \frac{1}{\sqrt{2}} & \sqrt{2}/6 & -2/3 \\ 0 & 2\sqrt{2}/3 & 1/3 \end{bmatrix} 
  \begin{bmatrix} -3 & 0 & 0 \\ 0 & -3 & 0 \\ 0 & 0 & 15 \end{bmatrix}
  \begin{bmatrix}  -\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} & 0 \\ \sqrt{2}/6 & \sqrt{2}/6  & 2\sqrt{2}/3 \\ -2/3 & -2/3 & 1/3 \end{bmatrix} \]

\end{frame}


\begin{frame}{The Spectral Theorem}
  The set of eigenvalues of $A$ is sometimes called the \alert{spectrum} of $A$.
  
  \begin{theorem}[The Spectral Theorem]
    If $A$ is a \alert{symmetric} $n\times n$ matrix, then $A$ is \alert{orthogonally diagonalizable} (with real eigenvalues).
  \end{theorem}
  
  There exist \alert{orthogonal} $\colorr{P}=\begin{bmatrix}\u_1 & \ldots & \u_n\end{bmatrix}$
  and \colorb{diagonal} $\colorb{D}=\begin{bmatrix} \lambda_1 & \ldots & 0 \\ 
                                                    \vdots & \ddots & \vdots \\
                                                    0 & \ldots & \lambda_n 
                                    \end{bmatrix}$ such that
  \begin{align*}
    A & = \colorr{P}\colorb{D}\colorr{P^T} = 
      \begin{bmatrix}\u_1 & \ldots & \u_n\end{bmatrix}
      \begin{bmatrix} \lambda_1 & \ldots & 0 \\ 
                      \vdots & \ddots & \vdots \\
                      0 & \ldots & \lambda_n 
      \end{bmatrix}
      \begin{bmatrix}\u_1^T \\ \vdots \\ \u_n^T\end{bmatrix}
    \onslide<2->{
      = \lambda_1 \u_1 \u_1^T + \lambda_2 \u_2 \u_2^T + \ldots + \lambda_n \u_n \u_n^T.
    }
  \end{align*}
  
  \onslide<3->{
    The matrices $\u_i \u_i^T$ are $n\times n$ matrices, rank $1$, and are orthogonal projection matrices.
    \smallskip
    
    The product $\u \v^T$ of $\u,\v\in\mathbb{R}^n$ is sometimes called the \emph{outer product}, and has rank $\le 1$.
  }
\end{frame}

\end{document}
