\documentclass[xcolor=dvipsnames,aspectratio=169,t]{beamer}
  % t means frames are vertically centered to the top
\usepackage{slides-header}
\title{Inner Product Spaces}

\begin{document}
\maketitle

\begin{frame}{The Dot Product}
  \bbox
  Let $\u$ and $\v$ denote two vectors in $\R^n$. The dot product of $\u$ and $\v$ is
  \[ \u \cdot \v = \u^T \v = \begin{bmatrix} u_1 & u_2 & \ldots & u_n \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix} =  u_1v_1 + u_2 v_2 + \ldots + u_nv_n = \sum_{i=1}^n u_iv_i .\] 
  \ebox
  \medskip

  \bi
  \ii The dot product is used to find \alert{lengths} of vectors and \alert{distances} between vectors in $\R^n$.\smallskip
  \ii The dot product is used to determine if vectors in $\R^n$ are \alert{orthogonal}.\smallskip
  \ii Let's extend this concept more generally to any vector space $V$.
  \ei
\end{frame}


\begin{frame}{Inner Product}
  \begin{definition}
  An \alert{inner product} on a vector space $V$ is a function that associates to each pair of vectors $\u$ and $\v$ in $V$ a \colorb{real number} that we denote $\langle \u,\v \rangle$.
  An inner product satisfies the following properties for every $\u$, $\v$, and $\w$ in $V$ and scalar $c$:
  \smallskip

  \bb
  \ii $\langle \u,\v \rangle = \langle \v,\u \rangle$. \smallskip
  \ii $\langle \u + \w,\v \rangle = \langle \u,\v \rangle + \langle \w,\v \rangle$. \smallskip
  \ii $\langle c \u,\v \rangle = c \langle \u,\v \rangle$. \smallskip
  \ii $\langle \u,\u \rangle \geq 0$, and $ \langle \u,\u \rangle =0$ if and only if $\u = \mathbf{0}$. \medskip
  \ee

  A vector space with an inner product is called an \alert{inner product space}.
  \end{definition}
  \medskip

  \blue{Example.}
  The vector space $\R^n$ with the usual dot product is an inner product space.
\end{frame}


\begin{frame}{Verify Properties Example 1}
  \smallskip

  Let $V = \R^2$ and consider the function given by $\langle \u , \v \rangle = 2u_1v_1 + 3u_2v_2$. 
  Show $V$ with this function is an inner product space.
  \bigskip

  \pause
  We verify the properties of an \alert{inner product}:
  \medskip
  \bb
  \ii $\langle \u , \v \rangle = 2u_1v_1 + 3u_2v_2 =  2v_1u_1 + 3v_2u_2 = \langle \v , \u \rangle$.
  \medskip
  \ii $\langle \u + \w , \v \rangle = 2(u_1+w_1)v_1 + 3(u_2+w_2) v_2 =  2u_1v_1 + 2w_1v_1  + 3u_2v_2 + 3w_2v_2 = \langle \u , \v \rangle + \langle \w , \v \rangle$.%
  \medskip
  \ii  $\langle c\u , \v \rangle = 2(cu_1)v_1 + 3(cu_2) v_2 = c(2u_1v_1+3u_2v_2) = c \langle \u , \v \rangle$.
  \medskip
  \ii $\langle \u , \u \rangle = 2u_1^2 + 3u_2^2 \geq 0$, and equality if and only if $u_1=u_2 =0$.
  \ee
\end{frame}

\begin{frame}{Verify Properties Example 2}
  \smallskip
  
  Let $V = \R^2$ and consider the function given by $\langle \u , \v \rangle = u_1v_2 + u_2v_1$. Determine whether $V$ with this function is an inner product space.
  \smallskip

  \pause
  \bb
  \ii $\langle \u , \v \rangle = u_1v_2 + u_2v_1 = v_1u_2 + v_2u_1 =  \langle \v , \u \rangle$. 
  \medskip
  \ii $\langle \u + \w , \v \rangle = (u_1+w_1)v_2 + (u_2+w_2)v_1 = (u_1v_2 + u_2v_1)+(w_1v_2+w_2v_1) = \langle \u , \v \rangle + \langle \w , \v \rangle$.%
  \medskip
  \ii $\langle c \u , \v \rangle = (cu_1)v_2 + (cu_2)v_1 = c (u_1v_2 + u_2v_1) = c \langle \u, \v \rangle$.
  \medskip
  \pause
  \ii Consider the vector $\u = \begin{bmatrix} 1\\ 0 \end{bmatrix}$. We have
  \[ \langle \u , \u \rangle = (1)(0)+(0)(1) = 0 \quad \mbox{but} \quad \u \ne \mathbf{0}. \]
  The last property is \alert{not} satisfied.
  \ee
  \bigskip

  $\R^2$ with the operation defined above is \alert{NOT} an inner product space.
\end{frame}


% \begin{frame}{Inner Product for $\mathbb{C}^n$}
% \smallskip
% 
%   Consider $z=a+bi$ as a vector in $\mathbb{C}^1$, where $a,b\in\R$.
%   What is the length of $z$?
%   \medskip
%   
%   \pause
%   If we use the dot product, $z\cdot z = z^2 = (a^2-b^2) + (2ab)i$.  This is \alert{not real} in general.
%   \bigskip
%   
%   \pause
%   Note that $|z|=a^2+b^2=\bar{z} z$, which is \alert{real}.
%   So for $\u,\v\in\mathbb{C}^n$, we define 
%   \[
%     \langle \u,\v\rangle = \bar u_1 v_1 + \bar u_2 v_2 + \ldots + \bar u_n v_n.
%   \]
%   
%   \pause
%   Let $A^H$ (or $A^*$) denote the \alert{conjugate transpose}.
%   Then we can write $\langle \u,\v \rangle = \u^H \v$.
%   \medskip
%   
%   \pause
%   \centering
%   \renewcommand{\arraystretch}{1.2}
%   \begin{tabular}{r|c|c}
%     & $\R^n$ & $\mathbb{C}^n$ \\\hline
%     inner product $\langle \u,\v \rangle=$ & $\u^T \v$ & $\u^H \v$ \\\hline
%     cols of square matrix are orthonormal & orthogonal matrix & unitary matrix \\
%                                           & $Q^T Q=I$ & $U^H U = I$ \\\hline
%     $A$ equal to (conjugate) transpose    & symmetric & Hermitian \\
%                                           & $A=A^T$   & $A=A^H$
%   \end{tabular}
%   
% \end{frame}


\begin{frame}{Inner Products on Polynomial Vector Spaces}
  \smallskip

  Recall $\mathbb{P}_2$ denotes the vector space of all polynomials of degree at most  2.
  For any two vectors $p(t)$ and $q(t)$ in $\mathbb{P}_2$, let
  \vspace*{-1.3em}
  
  \[ \qquad \langle p(t) , q(t) \rangle = p(0)q(0) + p(1)q(1) + p(2)q(2). \]
  Determine whether $\mathbb{P}_2$  with this function is an \alert{inner product} space.
  \bigskip

  \onslide*<2>{
  \bb
  \ii {\small $\langle p(t) , q(t) \rangle = p(0)q(0) + p(1)q(1) + p(2)q(2) = q(0)p(0) + q(1)p(1) + q(2)p(2) =  \langle q(t) , p(t) \rangle$. }
  \medskip

  \ii ~
  \vspace*{-3.1em}
  
  {\small \begin{align*} \langle \alert{p(t)+r(t)} , q(t) \rangle &= \alert{(p(0)+r(0))}q(0) + \alert{(p(1)+r(1))}q(1) + \alert{(p(2)+r(2))}q(2)\\
  & = \big( \alert{p(0)}q(0) + \alert{p(1)}q(1) + \alert{p(2)}q(2) \big) + \big( \alert{r(0)}q(0) + \alert{r(1)}q(1) + \alert{r(2)}q(2) \big) \\
  &=  \langle \alert{p(t)} , q(t) \rangle + \langle \alert{r(t)} , q(t) \rangle .
  \end{align*}}
  \ee
  }
  \onslide*<3->{
  \bb 
  \addtocounter{enumi}{2}

  \ii ~
  \vspace*{-3.5em}
  
  \begin{align*} \langle \blue{c} p(t) , q(t) \rangle 
  &= (\blue{c}p(0))q(0) + (\blue{c}p(1))q(1) + (\blue{c}p(2))q(2)\\
  &= \blue{c} \big(p(0)q(0) + p(1)q(1) + p(2)q(2) \big) \\
  &= \blue{c} \langle p(t), q(t) \rangle \end{align*}

  \ii $\langle p(t) , p(t) \rangle = p(0)p(0) + p(1)p(1)+p(2)p(2) = (p(0))^2 +  (p(1))^2 + (p(2))^2 \geq 0$.
  \medskip
  
  \onslide*<4->{
  We have equality when $p(0)=p(1)=p(2)=0$. If a polynomial with at most degree two has three distinct zeros, then the polynomial must be equal to zero for all $t$.
  
  This means $\langle p(t) , p(t) \rangle =0$ if and only if $p(t) = 0 +0t+ 0t^2 = \mathbf{0}$.
  }
  \ee
  }
\end{frame}


\begin{frame}{Lengths, Distances, and Orthogonality}

  \begin{definition}
    Let $V$ be an inner product space.  \smallskip
    \bi
    \ii We define the \alert{length} (or \alert{norm}) of a vector $\v$ to be $\| \v \| = \sqrt{\langle \v , \v \rangle}$. 
        %Equivalently, $\| \v \|^2 = \langle \v , \v \rangle$.
        \smallskip
    \ii A \alert{unit vector} is a vector whose length is 1. \smallskip
    \ii The \alert{distance between vectors} $\u$ and $\v$ is $\| \u - \v \|$.  \smallskip
    \ii Vectors $\u$ and $\v$ are \alert{orthogonal} if and only if $\langle \u , \v \rangle =0$.  \smallskip
    \ei
  \end{definition}
  \medskip

  Our vector space has been equipped with a \alert{geometric} structure (we have defined distances and angles in our space).
  \medskip
  
  Nearly everything discussed about dot products and orthogonality for $\R^n$ carries over to inner product spaces more generally (including \colorb{orthonormal bases} and the \colorb{Gram--Schmidt process}).
\end{frame}


\begin{frame}{Polynomial Vector Space Example}
  Let $V = \mathbb{P}_4$ with the inner product 
  \[ \langle p(t) , q(t) \rangle = \alert{p(-2)q(-2)} + \colorb{p(-1)q(-1)} + \colorg{p(0)q(0)}+\colorp{p(1)q(1)}+p(2)q(2) .\]

  Compute lengths and orthogonality of vectors $p_1(t) =1$, $p_2(t) = t$, and $p_3(t) = t^2$ in  $\mathbb{P}_4$.
  \bigskip
  
  \pause
  We create column vectors by evaluating the polynomials at the values $t=\alert{-2}, \colorb{-1},  \colorg{0} , \colorp{1}, 2$:

  \[ p_1(t)=1 \mbox{\ \ has } \begin{bmatrix} \alert{1} \\ \colorb{1} \\ \colorg{1} \\ \colorp{1} \\ 1 \end{bmatrix}, \quad 
  p_2(t)=t \mbox{\ \ has } \begin{bmatrix} \alert{-2} \\ \colorb{-1} \\ \colorg{0} \\ \colorp{1} \\ 2 \end{bmatrix}, \quad
  p_3(t)=t^2 \mbox{\ \ has } \begin{bmatrix} \alert{4} \\ \colorb{1} \\ \colorg{0} \\ \colorp{1} \\ 4 \end{bmatrix} \]
\end{frame}

\begin{frame}
  \[ \langle p(t) , q(t) \rangle = \alert{p(-2)q(-2)} + \colorb{p(-1)q(-1)} + \colorg{p(0)q(0)}+\colorp{p(1)q(1)}+p(2)q(2) .\]

  \[ p_1(t)=1 \mbox{\ \ has } \begin{bmatrix} \alert{1} \\ \colorb{1} \\ \colorg{1} \\ \colorp{1} \\ 1 \end{bmatrix}, \quad 
  p_2(t)=t \mbox{\ \ has } \begin{bmatrix} \alert{-2} \\ \colorb{-1} \\ \colorg{0} \\ \colorp{1} \\ 2 \end{bmatrix}, \quad
  p_3(t)=t^2 \mbox{\ \ has } \begin{bmatrix} \alert{4} \\ \colorb{1} \\ \colorg{0} \\ \colorp{1} \\ 4 \end{bmatrix} \]

  \bb
  \pause
  \ii Compute the length of the vector $p_1(t) = 1$. \smallskip
  
  \pause\qquad\colorb{ $ \| p_1(t) \| = \sqrt{\langle 1 , 1 \rangle} = \sqrt{1^2 + 1^2 + 1^2 + 1^2 + 1^2} =  \sqrt{5} $} \ms
  
  \pause
  \ii Are the vectors $p_1(t)=1$ and $p_2(t) = t$ orthogonal? \smallskip
  
  \pause\qquad\colorb{$\langle 1 , t \rangle =  (1)(-2)+(1)(-1)+(1)(0)+(1)(1)+(1)(2) = 0$} \quad $\Rightarrow$ \alert{orthogonal} \ms
  
  \pause
  \ii Are the vectors $p_1(t)=1$ and $p_3(t) = t^2$ orthogonal? \smallskip
  
  \pause\qquad\colorb{ $\langle 1 , t^2 \rangle =  (1)(4)+(1)(1)+(1)(0)+(1)(1)+(1)(4) = 10$} \quad $\Rightarrow$ \alert{not} orthogonal
  \ee
\end{frame}


\begin{frame}{Producing an Orthogonal Basis for a Polynomial Vector Space}
  Apply the \alert{Gram--Schmidt process} to produce an \alert{orthogonal basis} for the subspace $V$ of $\blue{\mathbb{P}_4}$ spanned by the vectors $p_1(t)=1, p_2(t)=t$, and $p_3(t)=t^2$.
  \smallskip

  \pause
  First we note the following inner products (\blue{using the inner product on $\mathbb{P}_4$}):
  \vspace*{-1em}
  
  \[ \langle 1 , 1 \rangle = 5,  \ \ \langle t , t \rangle = 10, \ \ \langle t^2 , t^2 \rangle = 34, \ \ \langle 1 , t \rangle = 0 , \ \ \langle 1 , t^2 \rangle = 10, \ \  \langle t , t^2 \rangle = 0. \]

  \pause
  \bi 
  \ii Our first vector is $\v_1 = p_1(t) = 1$.
  \medskip
  
  \pause
  \ii Next $\v_2 = p_2(t) - \proj_{\v_1} p_2(t) = t - 0 = t$.
  \vspace*{-1em}

  \[ \proj_{\v_1} p_2(t) = c_1 \cdot 1 = 0   \quad \mbox{where} \quad 
  c_1 = \frac{\langle t , 1 \rangle}{\langle 1 , 1 \rangle} = 0. \]

  \pause
  \ii The third (and final) vector $\v_3 = p_3(t) - \proj_{\{\v_1 , \v_2\}} p_3(t) = t^2 - 2$.
  \vspace*{-1em}

  {\small \[ \proj_{\{1, t\}} p_3(t) = c_1 \cdot 1 + c_2 \cdot t= (2)(1)-(0)(t) = 2 \mbox{ where }
  c_1 = \frac{\langle t^2,1 \rangle}{\langle 1,1 \rangle} = \frac{10}{5}, \quad
  c_2 = \frac{\langle t^2,t \rangle}{\langle t,t \rangle} = 0.  \]}
  \ei
  
  \pause
  Our \alert{orthogonal basis} is $\{1,t,t^2-2\}$.  Note this depends on the \blue{inner product of $\mathbb{P}_4$}.
\end{frame}


\begin{frame}{An Inner Product on Functional Vector Spaces}
  \smallskip

  Let $\mathcal{C}\lbrack a , b \rbrack$ denote the set of all \alert{continuous functions} on the closed interval $a \leq t \leq b$.

  \bi
  \ii Using the typical addition of functions and scalar multiplication, we can verify that $\mathcal{C}\lbrack a , b \rbrack$ is a vector space.
  \ii For two functions $f$ and $g$ in $\mathcal{C}\lbrack a , b \rbrack$, we define the following inner product:
  \vspace*{-1em}
  
  \[ \langle f , g \rangle = \int_a^b f(t)g(t) \, dt\]
  \ei

  Show $\mathcal{C}\lbrack a , b \rbrack$ with the operation defined above is an \alert{inner product space}.
  \medskip

  \pause
  \bb
  \ii $\langle f , g \rangle = \int_a^b f(t)g(t) \, dt = \int_a^b g(t)f(t) \, dt = \langle g , f \rangle$. \smallskip
  \ii $\langle \alert{f + h}  , g \rangle =  \int_a^b \left( \alert{(f(t)+ h(t))}  g(t) \right) \, dt  =   \int_a^b  \alert{f(t)}g(t) \, dt +   \int_a^b \alert{h(t)}g(t) \, dt  = \langle \alert{f} , g \rangle + \langle \alert{h} , g \rangle$.\smallskip%
  \ii $\langle \alert{c}f , g \rangle = \int_a^b (\alert{c}f(t))g(t) \, dt = \alert{c} \int_a^b f(t)g(t) \, dt = \alert{c} \langle f, g \rangle$. \smallskip
  \ii $\langle f , f \rangle =  \int_a^b (f(t))^2 \, dt \geq 0$ and equality if and only if $f(t) = 0$.
  \ee
\end{frame}


\begin{frame}{Computing Inner Products of Continuous Functions}
  \smallskip
  
  Consider the inner product space $\mathcal{C}\lbrack 0 , 1 \rbrack$ with the usual inner product $\langle f , g \rangle = \int_a^b f(t)g(t) \, dt$.
  \smallskip
  
  Compute $\langle 1+t^2, 4t \rangle$.

  \pause
  \begin{align*}
    \langle 1+t^2, 4t \rangle &= \int_0^1 \bigg( (1+t^2)4t \bigg)  \, dt \\
    &=  \int_0^1 (4t+4t^3)  \, dt \\
    &= \left. (2t^2 + t^4) \right|_0^1 \\
    &= 3.
  \end{align*}
  Thus, $1+t^2$ and $4t$ are \alert{not orthogonal} in $\mathcal{C}\lbrack 0 , 1 \rbrack$.
\end{frame}


\begin{frame}{Fourier Analysis}
\medskip
  
  A \alert{basis} for $\mathcal{C}[0,2\pi]$ is $\{1\} \cup \left\{\cos(mt) : m=1,2,\ldots\right\} \cup \left\{\sin(mt) : m=1,2,\ldots\right\}$.
  \medskip
  
  \onslide<2->{
  In fact, this is an \alert{orthogonal basis}.
  }%
  \onslide*<2>{%
  For example, where $m\ne n$ are positive integers,
  \begin{align*}
    \langle \cos(mt),\cos(nt) \rangle & = \int_0 ^{2\pi} \cos(mt) \cos(nt) \ dt \\
    &= \frac{1}{2} \int_0 ^{2\pi} \big[ \cos(mt+nt) + \cos(mt-nt) \big] \ dt \\
    &= \frac{1}{2} \left.\left[\frac{\sin(mt+nt)}{m+n}+\frac{\sin(mt-nt)}{m-n}\right]\right|_0 ^{2\pi} =0.
  \end{align*}
  }%
  \onslide*<3->{%
  Any $f\in \mathcal{C}[0,2\pi]$ can be represented by a \alert{Fourier series}
  \[ f(t) = \frac{a_0}{2} + %\sum_{m=1}^\infty \big(a_m \cos(mt) + b_m \sin(mt) \big), \]
  \sum_{m=1}^\infty a_m \cos(mt) + \sum_{m=1}^\infty b_m \sin(mt), \]
  where for $m\ge 1$,
  \[ a_m = \frac{\langle f,\cos(mt) \rangle }{\langle \cos(mt),\cos(mt) \rangle}
    = \frac{1}{\pi} \int_0 ^{2\pi} f(t) \cos(mt) \ dt, \quad \text{etc}.
  \]
  \smallskip
  }%
  \onslide*<4->{%
  
  Let $W$ be spanned by 
  $\{1\} \cup \left\{\cos(mt) : m=1,\ldots,n\right\} \cup \left\{\sin(mt) : m=1,\ldots,n\right\}$.
  \smallskip
  
  What is the \alert{best approximation} of $f$ by elements of $W$?
  
  \hfill\emph{Python examples in Jupyter Notebook.}
  }

  
\end{frame}

\end{document}
