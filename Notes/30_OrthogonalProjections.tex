\documentclass[xcolor=dvipsnames,aspectratio=169,t]{beamer}
  % t means frames are vertically centered to the top
\usepackage{slides-header}
\title{Orthogonal Projections}

\begin{document}
\maketitle

\begin{frame}{Orthogonal Projection onto a Subspace}
  \medskip

  \begin{columns}[T]
  \column{0.7\tw}
  Given a subspace $W$ of $\R^n$, in many instances it is useful to decompose a vector $\y$ in $\R^n$ into the sum of two vectors $\y = \hat{\y} + \z$:%
  \medskip
  
  \bi
  \ii  The \alert{orthogonal projection of $\y$ onto $W$} is $\hat{\y}=\mbox{proj}_W \y$.
  \smallskip
  \ii The \alert{component of $\y$ orthogonal to $W$} is $\z = \y - \hat{\y}$.
  \ei

  \column{0.3\tw}
  \scalebox{.85}{\input{images/fig-orthogonal-projection}}
  \end{columns}
  \bigskip

  \pause
  \begin{theorem}[The Orthogonal Decomposition Theorem]
  Let $W$ be a subspace of $\R^n$.
  Then each $\y$ in $\R^n$ can be written \alert{uniquely} in the form  $\y = \hat{\y} + \z$,
  \smallskip
  
  where $\hat{\y}$ is in $W$ and $\z$ is in $W^{\perp}$.
  If $\left\{ \u_1, \u_2, \ldots , \u_p \right\}$ is an \blue{orthogonal basis of $W$}, then
  \[ \hat{\y} = c_1 \u_1 + c_2 \u_2 + \ldots + c_p \u_p \mbox{ with weights } c_i = \frac{\y \cdot \u_i}{\u_i \cdot \u_i}, \quad \mbox{ and } \z = \y - \hat{\y}.\]
  \end{theorem}
\end{frame}


\begin{frame}{Orthogonal Decomposition Theorem}

  \begin{theorem}[The Orthogonal Decomposition Theorem]
  Let $W$ be a subspace of $\R^n$.
  Then each $\y$ in $\R^n$ can be written \alert{uniquely} in the form  $\y = \hat{\y} + \z$,
  \smallskip
  
  where $\hat{\y}$ is in $W$ and $\z$ is in $W^{\perp}$.
  If $\left\{ \u_1, \u_2, \ldots , \u_p \right\}$ is an \blue{orthogonal basis} of $W$, then
  \vspace*{-.5em}
  
  \[ \hat{\y} = c_1 \u_1 + c_2 \u_2 + \ldots + c_p \u_p \mbox{ with weights } c_i = \frac{\y \cdot \u_i}{\u_i \cdot \u_i}, \quad \mbox{ and } \z = \y - \hat{\y}.\]
  \end{theorem}
  
  \pause
  \blue{Proof.}
  \onslide*<2-5>{%
  Note that $\hat{\y}$ is in $W$, and that $\hat{\y}$ and $\z$ sum to $\y$.
  It remains to show that $\z$ is in $W^\perp$.
  \begin{align*}
    \onslide<2-5>{ \z \cdot \u_i 
                   = (\y-\hat{\y})\cdot \u_i
                   &= (\y \cdot \u_i) - \hat{\y} \cdot \u_i \\
                   &= (\y \cdot \u_i) - \left[ c_1 \u_1 + c_2 \u_2 + \ldots + c_p \u_p \right] \cdot \u_i \\ }
    \onslide<3-5>{ &= (\y \cdot \u_i) - c_1 (\u_1 \cdot \u_i) - \ldots - c_i (\u_i \cdot \u_i) - \ldots -c_p (\u_p\cdot \u_i) \\ }
    \onslide<4-5>{ &= (\y \cdot \u_i) - c_i (\u_i \cdot \u_i)
     = (\y \cdot \u_i) - \left(\frac{\y \cdot \u_i}{\u_i \cdot \u_i} \right) (\u_i \cdot \u_i) = 0. }
  \end{align*}
  }
  \onslide*<5>{
  \vspace*{-1em}
  
  Since $\z$ is orthogonal to each $\u_i$ in the basis, $\z$ is in $W^\perp$ by a previous theorem.
  }
  
  \onslide*<6->{
    For uniqueness, suppose that $\y=\hat\y+\z=\hat\y_1+\z_1$.
    Then $\hat\y-\hat\y_1=\z_1-\z$.
    \smallskip
    
  }
  \onslide*<7->{
  But $\hat{\y}-\hat{\y}_1\in W$ and $\z_1-\z \in W^\perp$, 
  which implies that $\hat{\y}-\hat{\y}_1=\z_1-\z=\mathbf{0}$.
  \smallskip
  
  Thus, $\hat\y_1=\hat\y$ and $\z_1=\z$.
  Therefore, $\hat\y$ and $\z$ are the only vectors satisfying the conditions.
  
  \hfill\blue{$\qed$}
  }
\end{frame}


\begin{frame}{Example}
  \smallskip

  Let $W = \Span \left\{ \u_1 , \u_2 \right\}$ where $\u_1 = \begin{bmatrix} 0 \\ 3 \\ 0 \end{bmatrix}$ and $\u_2 = \begin{bmatrix} -2 \\ 0 \\ 4 \end{bmatrix}$. Write $\y = \begin{bmatrix} -1 \\ 3 \\ 6 \end{bmatrix}$ as a \alert{sum} of a \blue{vector in $W$} and a vector \blue{orthogonal to $W$}.
  \medskip

  \pause
  The orthogonal projection of $\y$ onto $W$ is $\hat{\y} = c_1 \u_1 + c_2 \u_2$ where we have weights
  \bigskip
  
  {\small
  \qquad
  $c_1 =  \displaystyle\frac{\y \cdot \u_1}{\u_1 \cdot \u_1} = \frac{9}{9} = 1$ and 
  $c_2 = \displaystyle\frac{\y \cdot \u_2}{\u_2 \cdot \u_2} = \frac{26}{20} = \frac{13}{10}$. 
  \bigskip

  This gives
  
  \vspace{-0.7em}

  \[ \hat{\y} = (1)\u_1 +  \left( \frac{13}{10}\right)  \u_2 = \begin{bmatrix} 0 \\ 3 \\ 0 \end{bmatrix} + \frac{13}{10} \begin{bmatrix} -2 \\ 0 \\ 4 \end{bmatrix} = \begin{bmatrix} -13/5 \\ 3 \\ 26/5 \end{bmatrix}, \]
  and thus

  \vspace{-0.3em}
  
  \[ \z = \y - \hat{\y} = \begin{bmatrix} -1 \\ 3 \\ 6 \end{bmatrix} - \begin{bmatrix} -13/5 \\ 3 \\ 26/5 \end{bmatrix} = \begin{bmatrix} 8/5 \\ 0 \\ 4/5 \end{bmatrix}. \]
  }
\end{frame}


\begin{frame}{The Best Approximation Theorem}
  \begin{theorem}[The Best Approximation Theorem]
  Let $W$ be a subspace of $\R^n$, let $\y$ be any vector in $\R^n$, and let $\hat{\y}$ be the orthogonal projection of $\y$ onto $W$.
  Then $\hat{\y}$ is the \alert{closest point} in $W$ to $\y$, in the sense that
  \vspace*{-.5em}
  
  \[ \| \y - \hat{\y} \| < \| \y - \v \| \quad \mbox{for all $\v \ne \y$ in $W$.}\] 
  \end{theorem}

  \begin{columns}[T]
  \column{0.5\tw}
  \onslide<2->{
  \blue{Proof.}
  
  For any $\v\in W$, $\hat{\y}-\v$ is orthogonal to $\y-\hat{\y}$.}

  \onslide<3->{
  By Pyth., $\|\y-\v\|^2 = \|\y-\hat{\y}\|^2 + \|\hat{\y}-\v\|^2$.

  \qquad \quad So $\|\y-\v\| > \|\y-\hat{\y}\|$ for $\v\ne \hat{\y}$. \hfill\blue{$\qed$}
  }
  \medskip

  \onslide<4->{
  \bi
  \ii The vector $\hat{\y}$  is called the \alert{best approximation to $\y$ by elements of $W$}.
  \ii The distance $\| \y - \hat{\y} \| = \| \z \|$ is the error of the approximation.
  \ei
  }
  \column{0.5\tw}
  \vspace{-0.15in}
  
  \begin{center}
  \begin{tikzpicture}
    \draw[fill=black!5] (-3.5,1.25)--(-1.75,2.25)--(2.25,-.25)--(.5,-1.25)--cycle;
    \node[anchor=north] at (-1,-.5) {$W$};
  
    \coordinate (origin) at (-2,1);
    \node[anchor=north east] at (origin) {$0$};
    
    \coordinate (y) at (0,2);
    \draw[very thick,black] (origin)--(y);
    \node[anchor=south] at (y) {$\y$};
    \node[circle,fill,inner sep=1.5pt] at (y) {};
    
    \coordinate (yhat) at (0,0);
    \draw[very thick,black] (origin)--(yhat);
    \draw[very thick,blue] (y)--(yhat);
    \draw[thick,black] (0,.2)--(.2,.2)--(.2,0);
    \node[anchor=east] at (-2,-.25) {$\|\y-\hat\y\|$};
    \draw[thick,dashed,-stealth] (-2,-.25)--(-.1,1);
    \node[anchor=north] at (yhat) {$\hat\y$};
    \node[circle,fill,inner sep=1.5pt] at (yhat) {};
    
    \coordinate (v) at (1,0);
    \draw[very thick,blue] (yhat)--(v);
    \draw[very thick,blue] (y)--(v);
    \node[anchor=north] at (v) {$\v$};
    \node[circle,fill,inner sep=1.5pt] at (v) {};
    \node[anchor=west] at (.4,1.3) {$\|\y-\v\|$};
    \node[anchor=west] at (1,-1.5) {$\|\hat\y-\v\|$};
    \draw[thick,dashed,-stealth] (1,-1.5)--(.5,-.1);
  \end{tikzpicture}
  \end{center}
  \end{columns}
\end{frame}


\begin{frame}{Example}
  \smallskip
  
  Let $\v_1  = \begin{bmatrix} 1 \\ 3 \\ -2 \end{bmatrix}$ and 
  $\v_2 = \begin{bmatrix} 5 \\ 1 \\ 4 \end{bmatrix}$. 
  Find the \alert{best approximation} of $\y = \begin{bmatrix} 1 \\ 3 \\ 5 \end{bmatrix}$ in the subspace $W = \Span \left\{ \v_1, \v_2 \right\}$.

  \bs

  \pause
  From the \alert{Best Approximation Theorem}, we know that the orthogonal projection of $\y$ onto $W$ is the closest point in $W$ to $\y$. The orthogonal projection of $\y$ onto $W$ is $\hat{\y} = c_1 \v_1 + c_2 \v_2$
  \smallskip
  
  where we have weights 
  {\small
  $c_1 = \displaystyle\frac{\y \cdot \v_1}{\v_1 \cdot \v_1} =  \frac{0}{14}$ and 
  $c_2 = \displaystyle\frac{\y \cdot \v_2}{\v_2 \cdot \v_2} = \frac{28}{42}=\frac{2}{3}$.
  }
  \bigskip

  Therefore, the best approximation for $\y$ in $W$ is
  \[ \hat{\y} = (0) \begin{bmatrix} 1 \\ 3 \\ -2 \end{bmatrix} + \frac{2}{3} \begin{bmatrix} 5 \\ 1 \\ 4 \end{bmatrix} =  \begin{bmatrix} 10/3 \\ 2/3 \\ 8/3 \end{bmatrix} \quad \mbox{with} \quad 
  \|  \z \| = \left\| \begin{bmatrix} 1 \\ 3 \\ 5 \end{bmatrix} - \begin{bmatrix} 10/3 \\ 2/3 \\ 8/3 \end{bmatrix} \right\| = \left\| \begin{bmatrix} -7/3 \\ 7/3 \\ 7/3 \end{bmatrix} \right\| \approx 4.0415. \]
\end{frame}


\begin{frame}{Example}
  \smallskip

  Let $\u_1 = \begin{bmatrix} 1/3 \\ -2/3 \\ 2/3 \end{bmatrix}$ and $\u_2 = \begin{bmatrix} 2/3 \\ 2/3 \\ 1/3 \end{bmatrix}$ and define the subspace $W = \Span \left\{ \u_1 , \u_2 \right\}$  of $\R^3$.
  \smallskip
  
  Let $U = \begin{bmatrix} \u_1 & \u_2  \end{bmatrix}$.
  \alert{Compute} $U^TU$ and $UU^T$.
  \bigskip

  \pause
  We have 
  \[ U^TU=  \begin{bmatrix} 1/3 & -2/3 & 2/3 \\ 2/3 & 2/3 & 1/3 \end{bmatrix} 
  \begin{bmatrix} 1/3 & 2/3 \\ -2/3 & 2/3 \\ 2/3 & 1/3 \end{bmatrix} 
  = \begin{bmatrix} 1 & 0  \\ 0 & 1  \end{bmatrix}. \]
  So $U$ has \alert{orthonormal columns}!
  
  \pause
  \[ UU^T = \begin{bmatrix} 1/3 & 2/3 \\ -2/3 & 2/3 \\ 2/3 & 1/3 \end{bmatrix} \begin{bmatrix} 1/3 & -2/3 & 2/3 \\ 2/3 & 2/3 & 1/3 \end{bmatrix} = \begin{bmatrix} 5/9 & 2/9 & 4/9 \\ 2/9 & 8/9 & -2/9 \\ 4/9 & -2/9 & 5/9 \end{bmatrix} \]

  %We have
%\[ \hat{\y} = (\y \cdot \u_1) \u_1 + (\y \cdot \u_2) \u_2 = \frac{13}{3} \u_1 + \frac{2}{3} \u_2 = \begin{bmatrix} 23/3 \\ 41/3 \\ -6 \end{bmatrix} \]
\end{frame}


\begin{frame}{Example}
  \smallskip

  Let $\u_1 = \begin{bmatrix} 1/3 \\ -2/3 \\ 2/3 \end{bmatrix}$ and $\u_2 = \begin{bmatrix} 2/3 \\ 2/3 \\ 1/3 \end{bmatrix}$ and define the subspace $W = \Span \left\{ \u_1 , \u_2 \right\}$  of $\R^3$.
  \smallskip
  
  Let $U = \begin{bmatrix} \u_1 & \u_2  \end{bmatrix}$.
  \alert{Compute} $(UU^T)\y$ and $\hat{\y}=\mbox{\proj}_W \y$ for $\y=(1,-2,4)$.
  \bigskip

  \pause
  We have
  \[ (UU^T) \y = \begin{bmatrix} 5/9 & 2/9 & 4/9 \\ 2/9 & 8/9 & -2/9 \\ 4/9 & -2/9 & 5/9 \end{bmatrix}  \begin{bmatrix} 1 \\ -2 \\ 4 \end{bmatrix} = \begin{bmatrix} 17/9 \\ -22/9 \\ 28/9 \end{bmatrix}, \]

  \pause
  and
  \[ \hat{\y} = (\y \cdot \u_1) \u_1 + (\y \cdot \u_2) \u_2 = \frac{13}{3} \u_1 + \frac{2}{3} \u_2 = \frac{13}{3}  \begin{bmatrix} 1/3 \\ -2/3 \\ 2/3 \end{bmatrix} + \frac{2}{3} \begin{bmatrix} 2/3 \\ 2/3 \\ 1/3 \end{bmatrix} = \begin{bmatrix} 17/9 \\ -22/9 \\ 28/9 \end{bmatrix}. \]
\end{frame}


\begin{frame}{Orthogonal projection onto a Subspace using an Orthonormal Basis}
  The formula for $\hat{\y} = \mbox{proj}_W \y$ is simplified when the basis for $W$ is an \alert{orthonormal basis}.

  \begin{theorem}
  If $\left\{ \u_1,\u_2,\ldots,\u_p \right\}$ is an \alert{orthonormal basis} for a subspace $W$ of $\R^n$, then
  \[ \mbox{proj}_{W} \y = (\y \cdot \u_1) \u_1 + (\y \cdot \u_2) \u_2 + \ldots +  (\y \cdot \u_p) \u_p = UU^T\y \quad \mbox{for all $\y$ in $\R^n$,}\]
  where the matrix $U = \begin{bmatrix} \u_1 & \u_2 & \ldots & \u_p \end{bmatrix}$.
  \end{theorem}
  The matrix $UU^T$ is called the \alert{orthogonal projection matrix onto $W$}.
  \bigskip

  \pause
  \blue{Proof.}
  
  Note that $U^T\y = 
  \begin{bmatrix} \u_1^T \y \\ \u_2^T \y \\ \vdots \\ \u_p^T \y \end{bmatrix} =
  \begin{bmatrix} \u_1 \cdot \y \\ \u_2 \cdot \y \\ \vdots \\ \u_p \cdot \y \end{bmatrix}$.
  Thus, $U U^T \y= (\u_1 \cdot \y) \u_1 + (\u_2 \cdot \y) \u_2 + \ldots +  (\u_p \cdot \y) \u_p$.
\end{frame}

\end{document}
