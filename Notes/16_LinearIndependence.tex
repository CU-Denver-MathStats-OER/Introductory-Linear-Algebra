\documentclass[xcolor=dvipsnames,aspectratio=169,t]{beamer}
  % t means frames are vertically centered to the top
\usepackage{slides-header}
\title{Linear Independence}

\begin{document}
\maketitle

\begin{frame}{Redundancy when taking Span}

  We've seen that the \colorr{Span} of a set $S=\{\mathbf{v}_1,\mathbf{v}_2,\dots,\mathbf{v}_n\}$ of vectors does not change if we add a vector $\mathbf{w}$ that is a linear combination of $\mathbf{v}_1,\mathbf{v}_2,\dots,\mathbf{v}_n$.
  \bigskip
  
  So there is redundancy in $S$ for the \colorr{Span} if a vector $\mathbf{v}_j\in S$ can be written as a linear combination of the other vectors in $S$.
  \bigskip
  
  How can we tell if a vector in $S$ can be written as a linear combination of the other vectors?
  \bigskip
  \pause
  
  For \alert{each} vector $\mathbf{v}_j\in S$, we can solve
  \[
    x_1 \mathbf{v}_1 + x_2 \mathbf{v}_2 + \dots 
    + x_{j-1} \mathbf{v}_{j-1}
    + \overbrace{\phantom{x_{j-1} \mathbf{v}_{j-1}}}^{\text{$\mathbf{v_j}$ missing}}
    + x_{j+1} \mathbf{v}_{j+1} + \dots +x_n \mathbf{v}_n = \mathbf{v}_j.
  \]
  \pause
  
  \hfill This might require solving \alert{$n$ systems} of linear equations!
  
\end{frame}
  
\begin{frame}{Redundancy when taking Span}
  \textbf{\colorb{A better way:}}
  Solve the homogeneous system
  \[
    x_1 \mathbf{v}_1 + x_2 \mathbf{v}_2 + \dots 
    +x_n \mathbf{v}_n = \mathbf{0}.
  \]
  \pause
  
  \vspace*{-1em}
  Suppose there is a \alert{nontrivial} solution $c_1,c_2,\dots,c_n$ (ie, \alert{not all} $c_j$s are zero).
  
  In particular, suppose that $c_j\ne 0$.  Then we solve for $\mathbf{v}_j$:
  \begin{align*}
    c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \dots + c_j \mathbf{v}_j &+ \dots + c_n \mathbf{v}_n = \mathbf{0} \\[.5em]
    c_j \mathbf{v}_j &= - c_1 \mathbf{v}_1 - c_2 \mathbf{v}_2 + \dots - c_{j-1} \mathbf{v}_{j-1} - c_{j+1} \mathbf{v}_{j+1}  - \dots - c_n \mathbf{v}_n \\[.5em]
    \mathbf{v}_j &= -\frac{c_1}{c_j} \mathbf{v}_1 - \frac{c_2}{c_j} \mathbf{v}_2 - \dots - \frac{c_{j-1}}{c_j} \mathbf{v}_{j-1} - \frac{c_{j+1}}{c_j} \mathbf{v}_{j+1}  - \dots - \frac{c_n}{c_j} \mathbf{v}_n.
  \end{align*}
  \pause
  So $\mathbf{v}_j$ can be written as a \alert{linear combination} of the other vectors $\mathbf{v}_1,\dots,\mathbf{v}_{j-1},\mathbf{v}_{j+1},\dots,\mathbf{v}_n$.
  \smallskip
  
  \pause
  This is \alert{if and only if}: if $\mathbf{v_j}$ can be written as a linear combination of the other vectors, then there exists a nontrivial solution to the homogeneous system.
\end{frame}


\begin{frame}{Revisiting Vector Equations}

  {\small
  \bbox
  If $A$ is an $m \times n$ matrix with columns vectors $\a_1, \a_2, \ldots , \a_n$ (each $\a_i$ is in $\mathbb{R}^m$), the homogeneous matrix equation $A\mbf{x} = \mbf{0}$ has the same solution set as the vector equation
  \[ x_1  \a_1 + x_2  \a_2  + \ldots + x_n  \a_n = \mbf{0}, \]
  which has corresponding augmented matrix
  \[ \begin{bmatrix} \a_1 & \a_2 & \ldots & \a_n & \mbf{0} \end{bmatrix}. \]
  \ebox
  }

   \bi
   \ii Homogeneous linear systems always have a trivial solution $x_1=x_2= \ldots = x_n =0$.
   \ii If a non-trivial solution exists, then:
   \bi
   \ii At least one $x_j \ne 0$.
   \ii At least one $\mathbf{a}_j$ can be written as a linear combination of the other column vectors.
   \ei
   \ii If the trivial solution is the only solution, then it is not possible to write any column vector as a linear combination of the other column vectors. 
\ei 
   
\end{frame}

\begin{frame}[fragile]{Example}

  Determine whether the equation
  \[ x_1 \begin{bmatrix} 2 \\ 1 \\ 0 \end{bmatrix}  +
  x_2 \begin{bmatrix} -1 \\ 6 \\ 3 \end{bmatrix} +
  x_3 \begin{bmatrix}  6 \\ -10 \\ -6 \end{bmatrix} =
  \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix} \]
  has a nontrivial solution.
  \pause

  \[ \begin{bmatrix}
    2 & -1 & 6 & 0\\
    1 & 6 & -10 & 0\\
    0 & 3 & -6 & 0
  \end{bmatrix} \xrightarrow{\text{RREF}}
  \begin{bmatrix}
    1 & 0 & \alert{2} & 0 \\
    0 & 1 & \alert{-2} & 0\\
    0 & 0 & \alert{0} & 0 \end{bmatrix} \]
    \pause

  Since $x_3$ is a \alert{free variable}, nontrivial solutions exist!
  
\end{frame}

\begin{frame}{Interpreting the Solution}


  \begin{columns}

    \column{0.5\tw}
     Determine whether the equation
  \[ x_1 \begin{bmatrix} 2 \\ 1 \\ 0 \end{bmatrix}  +
  x_2 \begin{bmatrix} -1 \\ 6 \\ 3 \end{bmatrix} +
  x_3 \begin{bmatrix}  6 \\ -10 \\ -6 \end{bmatrix} =
  \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix} \]
  has a non-trivial solution.

  \ms
  
  Solution set is
  \[ \begin{bmatrix} x_1\\x_2\\x_3 \end{bmatrix} = \begin{bmatrix} -2x_3 \\ 2x_3 \\ x_3 \end{bmatrix} = x_3 \begin{bmatrix}-2 \\ 2\\ 1 \end{bmatrix} \]

  \column{0.5\tw}
  
So we have  
    \[ \alert{-2} \begin{bmatrix} 2 \\ 1 \\ 0 \end{bmatrix}  +
 \alert{2} \begin{bmatrix} -1 \\ 6 \\ 3 \end{bmatrix} +
  \alert{1} \begin{bmatrix}  6 \\ -10 \\ -6 \end{bmatrix} =
  \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix} \]

  which gives
    \[  \begin{bmatrix}  6 \\ -10 \\ -6 \end{bmatrix} =  \alert{2} \begin{bmatrix} 2 \\ 1 \\ 0 \end{bmatrix}  +
 \alert{-2} \begin{bmatrix} -1 \\ 6 \\ 3 \end{bmatrix} \]

\end{columns}

 
 \end{frame}

\begin{frame}{Linear Independence}

  \bbox
  Let $\left\{ \mathbf{v}_1 ,  \mathbf{v}_2, \ldots ,  \mathbf{v}_n \right\}$ denote a set of $n$ vectors each in $\mathbb{R}^m$. We say the set of vectors is \alert{linearly independent} if the vector equation $\dsty x_1  \mbf{v}_1 + x_2  \mbf{v}_2  + \ldots + x_n  \mbf{v}_n = \mbf{0}$ \alert{has only a trivial solution}.

  \ms

  Otherwise, a set of vectors $\left\{ \mathbf{v}_1 ,  \mathbf{v}_2, \ldots ,  \mathbf{v}_n \right\}$ is said to be \alert{linearly dependent} if there exists weights $c_1, c_2, \ldots, c_n$ \alert{not all zero} such that
  \[ c_1  \mbf{v}_1 + c_2  \mbf{v}_2  + \ldots + c_n  \mbf{v}_n = \mbf{0}. \]
  \ebox

  \begin{example}
    The set $\dsty \left\{ \begin{bmatrix} 2 \\ 1 \\ 0 \end{bmatrix} , \begin{bmatrix} -1 \\ 6 \\ 3 \end{bmatrix} ,
    \begin{bmatrix}  6 \\ -10 \\ -6 \end{bmatrix} \right\}$ is \alert{linearly dependent}.
    \end{example}
  
  \end{frame}

\begin{frame}{Example}

  \bigskip
  Determine whether the set of vectors $\dsty S = \left\{ \begin{bmatrix} 1 \\ 2 \end{bmatrix} ,  \begin{bmatrix} 3 \\ 4 \end{bmatrix}  \right\}$ is linearly independent.

  \begin{columns}

    \column{0.5\tw}
    
    \[ x_1 \begin{bmatrix} 1 \\ 2 \end{bmatrix} + x_2 \begin{bmatrix} 3 \\ 4 \end{bmatrix}  =  \begin{bmatrix} 0 \\ 0 \end{bmatrix}  \]

\column{0.5\tw}
        
    \[ \begin{bmatrix} 1 & 3  & 0\\
      2 & 4 & 0 \end{bmatrix} \xrightarrow{\text{RREF}}
    \begin{bmatrix}
      1 & 0 & 0 \\
      0 & 1 & 0 \end{bmatrix} \]

  \end{columns}
  \bigskip
  
  \pause
  
  There is a unique solution, the trivial solution $\mathbf{x} = \mathbf{0}$, so the vectors are \alert{linearly independent}.

  \bigskip
  
  \bbox
  The columns of matrix $A$ are linearly independent if and only if the matrix equation $A\mathbf{x} = \mathbf{0}$ has \alert{only the trivial solution}.
  \ebox

\end{frame}


\begin{frame}{The Invertible Matrix Theorem redux 2}
  Let $A$ be a square $n \times n$ matrix. Then all of the following statements are \alert{equivalent}.
  
  \begin{enumerate}[(a)]
    \item \alert{$A$ is an invertible matrix.}
    \item $A$ is row equivalent to the $n \times n$ identity matrix $I_n$.
    \item $A$ has $n$ pivots.
    \item The equation $A\x=\mathbf{0}$ has only the trivial solution.
    \item \blue{The columns of $A$ form a linearly independent set.}
    \item The linear transformation $\x\mapsto A\x$ mapping $\R^n$ into $\R^n$ is one-to-one.
    \item The equation $A\x=\b$ has at least one solution for each $\b$ in $\R^n$.
    \item The columns of $A$ span $\mathbb{R}^n$.
    \item The linear transformation $\x \mapsto A\x$ maps $\R^n$ onto $\R^n$.
    \item There is an $n \times n$ matrix $C$ such that $CA = I_n$.
    \item There is an $n \times n$ matrix $D$ such that $AD = I_n$.
    \item $A^T$ is an invertible matrix.
    \item \blue{$\det A \ne 0$}.
  \end{enumerate}
\end{frame}


\begin{frame}{What if $S$ contains $\mathbf{0}$?}

  \begin{theorem}
    If a set $S = \left\{ \mathbf{v}_1 ,  \mathbf{v}_2, \ldots ,  \mathbf{v}_n \right\}$ of vectors in $\mathbb{R}^m$ contains the \alert{zero vector $\mathbf{0}$}, then the set is linearly dependent.
  \end{theorem}
  \medskip

  \pause
  \blue{Proof.}
  
  Without any loss of generality, suppose $\mathbf{v}_1 = \mathbf{0}$. Then we see the equation
    \[\dsty x_1  \mbf{v}_1 + x_2  \mbf{v}_2  + \ldots + x_n  \mbf{v}_n = \mbf{0}\]
    has a non-trivial solution. 
    Namely, we set $x_1 = 1$ (or any non-zero value) and set $x_i=0$ for each $i>1$.  This gives
    \[\dsty 1  \mbf{0} + 0  \mbf{v}_2  + \ldots + 0 \mbf{v}_n =  \mbf{0} +  \mbf{0} + \ldots +  \mbf{0}  =  \mbf{0} .\]
    \hfill $\blue{\square}$
\end{frame}


\begin{frame}{Small Sets Vectors}

  {\small
  \bb
  \ii If possible, give an example of a set of 1 vector in $\mathbb{R}^3$ so the set is linearly independent. \vspace{0.5in}
  \ii If possible, give an example of a set of 2 vectors in $\mathbb{R}^3$ so the set is linearly independent.  \vspace{0.5in}
  \ii If possible, give an example of a set of 3 vectors in $\mathbb{R}^3$ so the set is linearly independent. \vspace{0.5in}
  \ii If possible, give an example of a set of 4 vectors in $\mathbb{R}^3$ so the set is linearly independent.
  \ee
  }
  
\end{frame}


\begin{frame}{Number of Vectors in Set $S$ and  the Dimension of the Vectors in $S$}

  \begin{theorem}
    Any set $\left\{ \v_1, \v_2, \ldots, \v_n \right\}$ of vectors in $\R^m$ is \alert{linearly dependent} if \alert{$n > m$}.
    That is, if a set contains more vectors than there are entries in each vector, then the set is linearly dependent. 
  \end{theorem}

  \pause
  \bbox
  The columns of matrix $A$ are linearly independent if and only if the matrix equation $A\x=\mathbf{0}$ has \alert{only the trivial solution}.
  \ebox

  \begin{proof}
    Let $\dsty A = \begin{bmatrix} \v_1 & \v_2 & \ldots & \v_n \end{bmatrix}$ 
    denote the $m \times n$ matrix.
    Then the equation $A\x=\mathbf{0}$ has $m$ equations and $n$ variables.
    \pause
    Since $n >m$, we have more variables than we have equations, so there must be at least one free variable.
    Thus $A\x = \mathbf{0}$ has a non-trivial solution, and the set $\left\{ \v_1, \v_2, \ldots,  \v_n \right\}$ is linearly dependent.
    \end{proof}
  
  \end{frame}

\begin{frame}{Characterization of Linearly Dependent Sets}
  \begin{theorem}
    An indexed set $S = \left\{ \v_1, \v_2, \ldots, \v_n \right\}$ of two or more vectors is \alert{linearly dependent} if and only if at least one of the vectors in $S$ is a \blue{linear combination of the others}. If fact, if $S$ is linearly dependent and $\v_1 \ne \mathbf{0}$, then some $\v_j$ (with $j > 1$) is a linear combination of the preceding vectors $\v_1, \v_2, \ldots, \v_{j-1}$.
  \end{theorem}
  
  \pause
  \begin{proof}
    \alert{Extra statement:} If $S$ is linearly dependent, then by definition there exists weights $c_1, c_2, \ldots, c_n$ not all zero such that $\dsty c_1  \v_1 + c_2  \v_2  + \ldots + c_n  \v_n = \mbf{0}$.
    Suppose $j$ is the largest index where $c_j \ne 0$. 
    \pause
    Then we can subtract all terms except for $c_j \v_j$ from both sides, and then divide both sides by $c_j$, giving
    \[  \v_j = -\frac{c_1}{c_j}  \v_1 -\frac{c_2}{c_j}  \v_2 - \ldots - \frac{c_{j-1}}{c_j}  \v_{j-1} -  \frac{0}{c_j}  \v_{j+1} - \ldots -  \frac{0}{c_j}  \v_{n}.\]
    Thus we see  $\v_j$ is a linear combination of the vectors $\v_1,\ldots,\v_{j-1}$.
  \end{proof}
  
\end{frame}
  
\end{document}
